{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioAuditore/TDA-for-Travelling-Salesman/blob/main/train_tsp_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCJCY5di_-BR"
      },
      "source": [
        "## Based on \"The Transformer Network for the Traveling Salesman Problem\"\n",
        "\n",
        "Xavier Bresson, Thomas Laurent, Feb 2021<br>\n",
        "\n",
        "Arxiv : https://arxiv.org/pdf/2103.03012.pdf<br>\n",
        "Talk : https://ipam.wistia.com/medias/0jrweluovs<br>\n",
        "Slides : https://t.co/ySxGiKtQL5<br>\n",
        "\n",
        "This code trains the transformer network by reinforcement learning.<br>\n",
        "Use the beam search code to test the trained network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE8vSQ6a__kR",
        "outputId": "34c53cd3-aba0-4694-b9cf-e5398bd660f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TDA-for-Travelling-Salesman'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 51 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (51/51), 104.13 MiB | 16.94 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MarioAuditore/TDA-for-Travelling-Salesman.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sBCshVpA2cp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/TDA-for-Travelling-Salesman')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5FqOgnGAEd-",
        "outputId": "a0ad765f-5da8-42a7-e9ff-79fab8c4f03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyconcorde@ git+https://github.com/jvkersch/pyconcorde\n",
            "  Cloning https://github.com/jvkersch/pyconcorde to /tmp/pip-install-rl8x1tcz/pyconcorde_601abf22ad3548519d5f7bac8d6efd49\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/jvkersch/pyconcorde /tmp/pip-install-rl8x1tcz/pyconcorde_601abf22ad3548519d5f7bac8d6efd49\n",
            "  Resolved https://github.com/jvkersch/pyconcorde to commit 8a6b193b79ebdf8f07e0b0635722b3b4edbc1560\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from pyconcorde@ git+https://github.com/jvkersch/pyconcorde) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pyconcorde@ git+https://github.com/jvkersch/pyconcorde) (1.25.2)\n",
            "Collecting tsplib95 (from pyconcorde@ git+https://github.com/jvkersch/pyconcorde)\n",
            "  Downloading tsplib95-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from tsplib95->pyconcorde@ git+https://github.com/jvkersch/pyconcorde) (8.1.7)\n",
            "Collecting Deprecated~=1.2.9 (from tsplib95->pyconcorde@ git+https://github.com/jvkersch/pyconcorde)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting networkx~=2.1 (from tsplib95->pyconcorde@ git+https://github.com/jvkersch/pyconcorde)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tabulate~=0.8.7 (from tsplib95->pyconcorde@ git+https://github.com/jvkersch/pyconcorde)\n",
            "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated~=1.2.9->tsplib95->pyconcorde@ git+https://github.com/jvkersch/pyconcorde) (1.14.1)\n",
            "Building wheels for collected packages: pyconcorde\n",
            "  Building wheel for pyconcorde (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyconcorde: filename=pyconcorde-0.1.0-cp310-cp310-linux_x86_64.whl size=2576468 sha256=169d9e414b4f4c115f1c8e06275ea5873f9b95a0c98d9f13ed33f81f28971746\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r4ut8iu8/wheels/be/9f/eb/e4f95e06e62f057a045679e4940e536d1b251d1ab4859c6dcc\n",
            "Successfully built pyconcorde\n",
            "Installing collected packages: tabulate, networkx, Deprecated, tsplib95, pyconcorde\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.9.0\n",
            "    Uninstalling tabulate-0.9.0:\n",
            "      Successfully uninstalled tabulate-0.9.0\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.21.0 requires tabulate>=0.9, but you have tabulate 0.8.10 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 networkx-2.8.8 pyconcorde-0.1.0 tabulate-0.8.10 tsplib95-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install 'pyconcorde @ git+https://github.com/jvkersch/pyconcorde'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsG4RZxt_-BV"
      },
      "outputs": [],
      "source": [
        "# ================\n",
        "# Libs\n",
        "# ================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Models\n",
        "from tsp_transformer.model import TSP_net, compute_tour_length\n",
        "\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "\n",
        "# visualization\n",
        "%matplotlib inline\n",
        "# from IPython.display import set_matplotlib_formats, clear_output\n",
        "# set_matplotlib_formats('png2x','pdf')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from concorde.tsp import TSPSolver # !pip install -e pyconcorde\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK1hku9S_-BX",
        "outputId": "e860cd2a-69f2-43b7-96e3-57df76e5c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU name: Tesla T4, gpu_id: 0\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "###################\n",
        "# Hardware : CPU / GPU(s)\n",
        "###################\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    gpu_id = '0'\n",
        "    device = torch.device(\"mps\")\n",
        "\n",
        "elif torch.cuda.is_available():\n",
        "    gpu_id = '0' # select a single GPU\n",
        "    # gpu_id = '2,3' # select multiple GPUs\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    gpu_id = -1 # select CPU\n",
        "\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OClcrWx_-BY",
        "outputId": "a5251448-f94a-44a6-f76f-f3c62e2efe54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'gpu_id': '0', 'nb_nodes': 10, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 20, 'batch_size': 128, 'nb_batch_per_epoch': 250, 'nb_batch_eval': 100, 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n"
          ]
        }
      ],
      "source": [
        "# ================\n",
        "# Hyper-parameters\n",
        "# ================\n",
        "\n",
        "class DotDict(dict):\n",
        "    def __init__(self, **kwds):\n",
        "        self.update(kwds)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = DotDict()\n",
        "args.gpu_id = gpu_id\n",
        "\n",
        "# TSP problem number of nodes\n",
        "args.nb_nodes = 10 # TSP20\n",
        "# args.nb_nodes = 50 # TSP50\n",
        "# args.nb_nodes = 100 # TSP100\n",
        "\n",
        "# Transformer parameters\n",
        "args.dim_emb = 128 # dimension of embeddings in transformer\n",
        "args.dim_ff = 512 # dimension of feed forward layers\n",
        "args.dim_input_nodes = 2\n",
        "args.nb_layers_encoder = 6\n",
        "args.nb_layers_decoder = 2\n",
        "args.nb_heads = 8\n",
        "\n",
        "#\n",
        "args.nb_epochs = 20 # number of epochs\n",
        "args.batch_size = 128 # batch size\n",
        "args.nb_batch_per_epoch = 250 # number of batches to generate on each epoch for training\n",
        "args.nb_batch_eval = 100 # number of batches to generate on each epoch for evaluation\n",
        "args.lr = 1e-4 # optimiser lr\n",
        "args.tol = 1e-3 # model should perform better w.r.t tolerance to be updated\n",
        "args.batchnorm = True  # if batchnorm=True  than batch norm is used\n",
        "# args.batchnorm = False # if batchnorm=False than layer norm is used\n",
        "args.max_len_PE = 1000\n",
        "\n",
        "print(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDtaarsA_-BZ"
      },
      "source": [
        "# Training\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veiaTFo1_-BZ",
        "outputId": "837c6d89-c9c2-4d7f-b298-fb4cee1034a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'gpu_id': '0', 'nb_nodes': 10, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 20, 'batch_size': 128, 'nb_batch_per_epoch': 250, 'nb_batch_eval': 100, 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "###################\n",
        "# Instantiate a training network and a baseline network\n",
        "###################\n",
        "try:\n",
        "    del model_train # remove existing model\n",
        "    del model_baseline # remove existing model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "model_train = TSP_net(args.dim_input_nodes,\n",
        "                      args.dim_emb,\n",
        "                      args.dim_ff,\n",
        "                      args.nb_layers_encoder,\n",
        "                      args.nb_layers_decoder,\n",
        "                      args.nb_heads,\n",
        "                      args.max_len_PE,\n",
        "                      batchnorm=args.batchnorm)\n",
        "\n",
        "model_baseline = TSP_net(args.dim_input_nodes,\n",
        "                         args.dim_emb,\n",
        "                         args.dim_ff,\n",
        "                         args.nb_layers_encoder,\n",
        "                         args.nb_layers_decoder,\n",
        "                         args.nb_heads,\n",
        "                         args.max_len_PE,\n",
        "                         batchnorm=args.batchnorm)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(torch.cuda.device_count() + \" cuda devices found, doing parallel training.\")\n",
        "    model_train = nn.DataParallel(model_train)\n",
        "    model_baseline = nn.DataParallel(model_baseline)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_train.parameters(), lr = args.lr)\n",
        "\n",
        "model_train = model_train.to(device)\n",
        "model_baseline = model_baseline.to(device)\n",
        "model_baseline.eval()\n",
        "\n",
        "print(args); print('')\n",
        "\n",
        "# Logs\n",
        "os.system(\"mkdir logs\")\n",
        "time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
        "file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n",
        "file = open(file_name,\"w\",1)\n",
        "file.write(time_stamp+'\\n\\n')\n",
        "for arg in vars(args):\n",
        "    file.write(arg)\n",
        "    hyper_param_val=\"={}\".format(getattr(args, arg))\n",
        "    file.write(hyper_param_val)\n",
        "    file.write('\\n')\n",
        "file.write('\\n\\n')\n",
        "plot_performance_train = []\n",
        "plot_performance_baseline = []\n",
        "all_strings = []\n",
        "epoch_ckpt = 0\n",
        "tot_time_ckpt = 0\n",
        "\n",
        "\n",
        "# # Uncomment these lines to re-start training with saved checkpoint\n",
        "# ====================================================================\n",
        "# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-25-00-n50-gpu0.pkl\"\n",
        "# checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "# epoch_ckpt = checkpoint['epoch'] + 1\n",
        "# tot_time_ckpt = checkpoint['tot_time']\n",
        "# plot_performance_train = checkpoint['plot_performance_train']\n",
        "# plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
        "# model_baseline.load_state_dict(checkpoint['model_baseline'])\n",
        "# model_train.load_state_dict(checkpoint['model_train'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
        "# del checkpoint\n",
        "# ====================================================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0AVLx11_-BZ"
      },
      "source": [
        "## Test nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTmQB-qQ_-Ba",
        "outputId": "bc3806cf-c91f-4c6a-b1df-c294ca1827f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nb of nodes : 10\n"
          ]
        }
      ],
      "source": [
        "###################\n",
        "# Small test set for quick algorithm comparison\n",
        "# Note : this can be removed\n",
        "###################\n",
        "\n",
        "save_1000tsp = False\n",
        "\n",
        "test_size = 10\n",
        "\n",
        "if save_1000tsp:\n",
        "    x = torch.rand(test_size, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
        "    print(x.size(),x[0])\n",
        "    data_dir = os.path.join(\"data\")\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    if args.nb_nodes==20 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp20\"))\n",
        "    if args.nb_nodes==50 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp50\"))\n",
        "    if args.nb_nodes==100 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp100\"))\n",
        "\n",
        "checkpoint = None\n",
        "\n",
        "if args.nb_nodes==20 : checkpoint = torch.load(\"data/1000tsp20.pkl\")\n",
        "if args.nb_nodes==50 : checkpoint = torch.load(\"data/1000tsp50.pkl\")\n",
        "if args.nb_nodes==100 : checkpoint = torch.load(\"data/1000tsp100.pkl\")\n",
        "\n",
        "if checkpoint is not None:\n",
        "    x_test_tsp = checkpoint['x'].to(device)\n",
        "    n = x_test_tsp.size(1)\n",
        "    print('nb of nodes :',n)\n",
        "else:\n",
        "    x_test_tsp = torch.rand(test_size, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
        "    n = x_test_tsp.size(1)\n",
        "    print('nb of nodes :',n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXsKSVkU_-Ba"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "23U8g_gl_-Ba",
        "outputId": "bb4a41df-c842-4c6c-9c98-874087fa9592"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [00:35<11:12, 35.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, epoch time: 0.519 min, tot time: 0.000day, L_train: 2.980, L_base: 3.906, L_test: 3.341, gap_train(%): -100.000, update: True\n",
            "Epoch: 1, epoch time: 0.383 min, tot time: 0.001day, L_train: 2.914, L_base: 2.984, L_test: 3.237, gap_train(%): -100.000, update: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [01:31<08:27, 29.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, epoch time: 0.385 min, tot time: 0.001day, L_train: 2.908, L_base: 2.912, L_test: 3.184, gap_train(%): -100.000, update: True\n"
          ]
        }
      ],
      "source": [
        "# ==================\n",
        "# Main training loop\n",
        "# ==================\n",
        "\n",
        "\n",
        "start_training_time = time.time()\n",
        "\n",
        "for epoch in tqdm(range(0, args.nb_epochs)):\n",
        "\n",
        "    # re-start training with saved checkpoint\n",
        "    # epoch += epoch_ckpt\n",
        "\n",
        "    # -------------------------\n",
        "    # Train model for one epoch\n",
        "    # -------------------------\n",
        "    start = time.time()\n",
        "    model_train.train()\n",
        "\n",
        "    for step in range(1, args.nb_batch_per_epoch + 1):\n",
        "\n",
        "        # generate a batch of random TSP instances\n",
        "        x = torch.rand(args.batch_size, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(batch_size, nb_nodes, dim_input_nodes)\n",
        "\n",
        "        # compute tours for model\n",
        "        tour_train, sumLogProbOfActions = model_train(x, deterministic=False) # size(tour_train)=(batch_size, nb_nodes), size(sumLogProbOfActions)=(batch_size)\n",
        "\n",
        "        # compute tours for baseline\n",
        "        with torch.no_grad():\n",
        "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
        "\n",
        "        # get the lengths of the tours\n",
        "        L_train = compute_tour_length(x, tour_train) # size(L_train)=(batch_size)\n",
        "        L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(batch_size)\n",
        "\n",
        "        # backprop\n",
        "        loss = torch.mean((L_train - L_baseline) * sumLogProbOfActions )\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    time_one_epoch = time.time()-start\n",
        "    time_tot = time.time()-start_training_time + tot_time_ckpt\n",
        "\n",
        "\n",
        "    # -----------------\n",
        "    # Evaluate train model and baseline on 10k random TSP instances\n",
        "    # -----------------\n",
        "    model_train.eval()\n",
        "    mean_tour_length_train = 0\n",
        "    mean_tour_length_baseline = 0\n",
        "\n",
        "    for step in range(0, args.nb_batch_eval):\n",
        "        # generate a batch of random tsp instances\n",
        "        x = torch.rand(args.batch_size, args.nb_nodes, args.dim_input_nodes, device=device)\n",
        "\n",
        "        # compute tour for model and baseline\n",
        "        with torch.no_grad():\n",
        "            tour_train, _ = model_train(x, deterministic=True)\n",
        "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
        "\n",
        "        # get the lengths of the tours\n",
        "        L_train = compute_tour_length(x, tour_train)\n",
        "        L_baseline = compute_tour_length(x, tour_baseline)\n",
        "\n",
        "        # L_tr and L_bl are tensors of shape (batch_size,). Compute the mean tour length\n",
        "        mean_tour_length_train += L_train.mean().item()\n",
        "        mean_tour_length_baseline += L_baseline.mean().item()\n",
        "\n",
        "    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n",
        "    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n",
        "\n",
        "    # evaluate train model and baseline and update if train model is better\n",
        "    update_baseline = mean_tour_length_train + args.tol < mean_tour_length_baseline\n",
        "    if update_baseline:\n",
        "        model_baseline.load_state_dict(model_train.state_dict())\n",
        "\n",
        "    # For new baseline compute TSPs for small test set\n",
        "    with torch.no_grad():\n",
        "        tour_baseline, _ = model_baseline(x_test_tsp.to(device), deterministic=True)\n",
        "    mean_tour_length_test = compute_tour_length(x_test_tsp, tour_baseline.to('cpu')).mean().item()\n",
        "\n",
        "    # For checkpoint\n",
        "    plot_performance_train.append([(epoch+1), mean_tour_length_train])\n",
        "    plot_performance_baseline.append([(epoch+1), mean_tour_length_baseline])\n",
        "\n",
        "    # Compute optimality gap\n",
        "    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692 - 1.0\n",
        "    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765 - 1.0\n",
        "    else: gap_train = -1.0\n",
        "\n",
        "\n",
        "    # ОНИ БЛЯТЬ ДЕБИЛЫ? ЧТО ТУТ ВООБЩЕ НАХУЙ ПРОСИХОДИТ? КАКОГО ХЕРА ОНИ ВЫШЕ ДЕЛЯТ НА РАНДОМНЫЕ ЧИСЛА В gap_train?????\n",
        "    # # Print and save in txt file\n",
        "    # mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
        "    #     epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline)\n",
        "    # print(mystring_min) # Comment if plot display\n",
        "    # file.write(mystring_min+'\\n')\n",
        "\n",
        "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f} min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
        "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline)\n",
        "    print(mystring_min) # Comment if plot display\n",
        "\n",
        "    # all_strings.append(mystring_min) # Uncomment if plot display\n",
        "    # for string in all_strings:\n",
        "    #     print(string)\n",
        "\n",
        "    # Saving checkpoint\n",
        "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'time': time_one_epoch,\n",
        "        'tot_time': time_tot,\n",
        "        'loss': loss.item(),\n",
        "        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item(), mean_tour_length_test],\n",
        "        'plot_performance_train': plot_performance_train,\n",
        "        'plot_performance_baseline': plot_performance_baseline,\n",
        "        'mean_tour_length_test': mean_tour_length_test,\n",
        "        'model_baseline': model_baseline.state_dict(),\n",
        "        'model_train': model_train.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKIJs494_-Bb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "tda_tsp",
      "language": "python",
      "name": "tda_tsp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}